\begin{lemma}[Grothendieck's identity]\label{lem:G_id}
	Let $u,v\in\mathbb{R}^d$ be unit vectors. Let $r\in\mathbb{R}^d$ be a random unit vector chosen from $O(d)$-invariant probability distribution on the unit sphere. Then
	\begin{enumerate}
		\item[i,] $\mathbb{P}[\sgn(u^\top r)\neq\sgn(v^\top r)]=\frac{\arccos(u^\top v)}{\pi}$
		\item[ii,] $\mathbb{E}[\sgn(u^\top r)\sgn(v^\top r)]=\frac{2}{\pi}\arcsin(u^\top v).$
	\end{enumerate}
\end{lemma}
\begin{proof}
	Assume that $u$ and $v$ are linearly dependent. Since both, $u$ and $v$, are unit vectors, that is, either $u=v$, then $\arccos(u^\top v) = \arccos(1)=0$ or $u=-v$, then $\arccos(u^\top v) = \arccos(-1) = \pi$.
	%Bild von arccos
	Conversely assume that $u$ and $v$ are linearly independent, i.\,e. $\operatorname{dim}(\spn\{u,v\})=2$. Now project $r$ orthogonally on the plane spanned by $u$ and $v$. This gives us a vector $s\in \spn\{u,v\}$ such that $u^\top r = u^\top s$, $v^\top r = v^\top s$. The unit vector $s/\norm{s}$ is uniformly distributed on the unit circle by the $O(d)$-invariance of the probability distribution. 
	
	\textcolor{red}{Bildchen und Begr√ºndung vom letzten Schritt}
	
	\begin{align*}
		\mathbb{E}[\sgn(u^\top r) \sgn(v^\top r)]
		&= 1\cdot\mathbb{P}[\sgn(u^\top r) = \sgn (v^top r )] - 1\cdot \mathbb{P}[\sgn(u^\top r) \neq \sgn(v^\top r)] \\
		&= 1 - 2\mathbb{P}[\sgn(u^\top r) \neq \sgn(v^\top r)] \\
		&= 1 - 2 \frac{\arccos(u^\top v)}{\pi} \\
		&= \frac{2}{\pi} \arcsin(u^\top v),
	\end{align*}
	because $\arcsin (t) = \arccos(t) = \pi/2$.
\end{proof}

\begin{lemma}[Krivine's trick]\label{lem:krivines_trick}
	Let $u_1,\dots,u_m$,$v_1,\dots,v_n\in S^{m+n-1}$ be given. Then there are $u_1^\prime,\dots,u_m^\prime$,$v_1^\prime,\dots,v_n^\prime\in S^{m+n-1}$ so that
	\begin{equation}
		\mathbb{E}[\sgn((u_i^\prime)^\top r)\sgn((v_j^\prime)^\top r)] = \beta u_i^\top v_j,
	\end{equation}		
	with $\beta = \frac{2}{\pi} \ln (1+\sqrt{2}).$
\end{lemma}

For the proof of \ref{lem:krivines_trick} we need to use the tensor product. \textcolor{red}{We have already seen the tensor product for complex vector spaces. Now we give an alternative definition for the $\mathbb{R}^n$.} The $\mathbb{R}^n$ is an $n$-dimensional Euclidean space with inner product \sclr{\cdot}{\cdot} and orthonormal basis $e_1,\dots,e_n$.
Then the \emph{k-th tensor product of $\mathbb{R}^n$} is denoted by $(\mathbb{R}^n)^{\tensor k}$ and it is a Euclidean  vector space of dimension $n^k$ with orthonormal basis $e_{i_1}\tensor e_{i_2} \tensor \cdots \tensor e_{i_k}$, $i_j\in\{1,\dots,n\}$. In particular
\begin{align}
	(e_{i_1}\tensor \cdots \tensor e_{i_k})^\top (e_{j_1}\tensor \cdots \tensor e_{j_k}) 
	&= \prod_{l=1}^k e_{i_l}^\top e_{j_l}\nonumber\\
	&=\begin{cases}
		1 & , \text{ if } i_l=j_l \text{ for all } l=1,\dots,n,\\
		0 & , \text{ otherwise},
	\end{cases} \label{eq:orthonormtensor}
\end{align}
and for $v\in\mathbb{R}^n$ with $v=v_1e_1+\cdots +v_ne_n$ we define $v^{\tensor k} \in (\mathbb{R}^n)^{\tensor k}$ by 
\begin{equation}
	v^{\tensor k} = (v_1e_1 + \cdots + v_ne_n) \tensor \cdots \tensor (v_1e_1 + \cdots + v_ne_n) = \sum_{i_1,\dots,i_k} v_{i_1}\cdots v_{i_k} e_{i_1}\tensor\cdots\tensor e_{i_k},
\end{equation}
where the last equation follows by the distributive law (identity ii, of the tensor product). 
Thus, for $v,w\in\mathbb{R}^n$ 
\begin{align}
	(v^{\tensor k})^\top w^{\tensor k}
	&\overset{\textcolor{white}{\eqref*{eq:orthonormtensor}}}{=} \sum_{i_1,\dots,i_k} v_{i_1}\cdots v_{i_k}\left(e_{i_1}\tensor\cdots\tensor e_{i_k}\right)^\top \sum_{j_1,\dots,j_k} w_{j_1}\cdots w_{j_k}(e_{j_1}\tensor\cdots\tensor e_{j_k}) \nonumber\\
	&\overset{\textcolor{white}{\eqref*{eq:orthonormtensor}}}{=} \sum_{i_1,\dots,i_k} v_{i_1}\cdots v_{i_k} \sum_{j_1,\dots,j_k} w_{j_1}\cdots w_{j_k} \left(e_{i_1}\tensor\cdots\tensor e_{i_k}\right)^\top (e_{j_1}\tensor\cdots\tensor e_{j_k}) \nonumber\\
	&\overset{\eqref{eq:orthonormtensor}}{=} \sum_{i_1,\dots,i_k} v_{i_1}\cdots v_{i_k}w_{i_1}\cdots w_{i_k} \nonumber\\
	&\overset{\textcolor{white}{\eqref*{eq:orthonormtensor}}}{=}(\sum_{i=1}^n v_iw_i)^k = (v^\top w)^k \label{eq:kth_tensor}
\end{align}
\begin{proof}
	Define the function $E: [-1,+1] \to [-1,+1]$ by $E(t)=\frac{2}{\pi}\arcsin(t)$. Due to Grothendieck's identity (Lemma \ref{lem:G_id}):
	\begin{align*}
		E((u_i^\prime)^\top v_j^\prime ) &= \mathbb{E}[\sgn((u_i^\prime)^\top r)\sgn((v_j^\prime)^\top r)]\\
		&\overset{!}{=}\beta u_i^\top v_j,
	\end{align*}
	where $r\in\mathbb{R}^d$ is a random unit vector chosen form the $O(d)$-invariant probability distribution on the unit sphere.\\
	
	Idea: To find $\beta,u_i^\prime,v_j^\prime$ we invert $E$:
	\[
		(u_i^\prime)^\top v_j^\prime = E^{-1} (\beta u_i^\top v_j)	
	\]
	with 
	\begin{align*}
		E^{-1}(t) &= \sin(\pi/2 \cdot t) \\
		&= \sum_{k=0}^\infty \underbrace{\frac{(-1)^{2k+1}}{(2k+1)!}\left(\frac{\pi}{2}\right)^{2k+1}}_{g_{2k+1}}  t^{2k+1}
	\end{align*}
	which is valid for all $t\in[-1,+1]$.
	
	Define the infinite-dimensional Hilbert space
	\begin{equation}
		H= \bigoplus_{r=0}^\infty (\mathbb{R}^{m+n})^{\tensor 2k+1}.
	\end{equation}
	\textcolor{red}{Begruenden, dass H gross genug ist?}
	
	Define $u_i^\prime, v_j^\prime\in H$ componentwise:
	\begin{align*}
		(u_i^\prime)_k &= \sgn(g_{2k+1}) \sqrt{\modul{g_{2k+1}}\beta^{2k+1}}\, u^{\tensor 2k+1} \\
		(v_j^\prime)_k &= \sqrt{\modul{g_{2k+1}}\beta^{2k+1}} \,v^{\tensor 2k+1}
	\end{align*}
	\textcolor{red}{woher sind u und v?}
	Then 
	\begin{align*}
		(u_i^\prime)^\top v_j^\prime &\overset{\textcolor{white}{\eqref*{eq:kth_tensor}}}{=} \sum_{k=0}^\infty g_{2k+1} \beta^{2k+1}(u^{\tensor 2k+1})^\top v^{\tensor 2k+1} \\
		&\overset{\eqref{eq:kth_tensor}}{=} \sum_{k=0}^\infty g_{2k+1} \beta^{2k+1} (u^\top v)^{2k+1} \\
		&\overset{\textcolor{white}{\eqref*{eq:kth_tensor}}}{=} E^{-1}(\beta u_i^\top v_j).
	\end{align*}
	Hence, $\beta$ is defined by the condition
	\[
		1 = (u_i^\prime)^\top u_i = (v_j^\prime)^\top v_j 
		%= \sum_{k=0}^\infty \modul{g_{2k+1}} \beta^{2k+1} 
		= \sum_{k=0}^\infty \frac{1}{(2k+1)!}\left(\frac{\pi}{2}\right)^{2k+1}\beta^{2k+1}=\sinh(\frac{\pi}{2}\beta)
	\]
	and
	\[
		\beta = \frac{2}{\pi} \arcsinh(1) = \frac{2}{\pi}\ln(1+\sqrt(2)),	
	\]
	since $\arcsinh (t) = \ln(t+\sqrt{t^2+1})$.
	\textcolor{red}{Gram matrix}
\end{proof}
\begin{dfn}
	For $A\in\mathbb{R}^{m\times n}$ define the quadratic program
	\begin{equation}
		\norm{A}_{\infty\to 1} = \max \left\{ \sum_{i=1}^m \sum_{j=1}^n A_{ij} x_i y_j : x_i^2=1, i=1,\dots,m, y_j^2=1, j =1,\dots,n \right\}.
	\end{equation}
\end{dfn}
\textcolor{red}{computing NP-hard}
\begin{dfn}
	SDP relaxation of $\norm{A}_{\infty\to1}$:
	\begin{align*}
		\operatorname{sdp}_{\infty\to 1} (A) = \max 
		&\sum_{i=1}^m\sum_{j=1}^n A_{ij} u_i^\top v_j\\
		&u_i,v_j\in\mathbb{R}^{m+n}\\
		&\norm{u_i}=1, i=1,\dots,m\\
		&\norm{v_j}=1, j=1,\dots,n.
	\end{align*}
\end{dfn}
\begin{theo}[Grothendieck's inequality] \label{eq:G_ineq}
	There exists a constant $K$ such that for all $A\in\mathbb{R}^{m\times n}$:
	\begin{equation}
		\norm{A}_{\infty\to 1} \leq \operatorname{sdp}_{\infty\to 1} (A) \leq K \norm{A}_{\infty\to 1}.
	\end{equation}
\end{theo}
\begin{proof}
	Again: approximation algorithm with randomized rounding
	\textcolor{red}{program}
	Expected quality of the outcome:
	\begin{align*}
		\norm{A}_{\infty\to 1} &\geq \mathbb{E}\left[\sum_{i=1}^m\sum_{j=1}^n A_{ij}x_ix_j\right]\\
		&= \sum_{i=1}^m\sum_{j=1}^n A_{ij} \mathbb{E}[\sgn((u_i^\prime)^\top r) \sgn((v_j^\prime)^\top r)] \\
		&=\sum_{i=1}^m\sum_{j=1}^n A_{ij}\beta u_i^\top v_j \\
		&=\beta \operatorname{sdp}_{\infty\to 1}(A),
	\end{align*}
	where $\beta = \frac{2\ln(1+\sqrt(2)}{\pi}$, thus $K_G\leq \beta^{-1}$.
\end{proof}
\newpage