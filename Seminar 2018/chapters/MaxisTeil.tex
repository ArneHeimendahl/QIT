\begin{lemma}[Grothendieck's identity]
	Let $u,v\in\mathbb{R}^d$ be unit vectors. Let $r\in\mathbb{R}^d$ be a random unit vector chosen from $O(d)$-invariant probability distribution on the unit sphere. Then
	\begin{enumerate}
		\item[i,] $\mathbb{P}[\sgn(u^\top r)\neq\sgn(v^\top r)]=\frac{\arccos(u^\top v)}{\pi}$
		\item[ii,] $\mathbb{E}[\sgn(u^\top r)\sgn(v^\top r)]=\frac{2}{\pi}\arcsin(u^\top v).$
	\end{enumerate}
\end{lemma}
\begin{proof}
	Assume that $u$ and $v$ are linearly dependent. Since both, $u$ and $v$, are unit vectors, that is, either $u=v$, then $\arccos(u^\top v) = \arccos(1)=0$ or $u=-v$, then $\arccos(u^\top v) = \arccos(-1) = \pi$.
	%Bild von arccos
	Conversely assume that $u$ and $v$ are linearly independent, i.\,e. $\operatorname{dim}(\spn\{u,v\})=2$. Now project $r$ orthogonally on the plane spanned by $u$ and $v$. This gives us a vector $s\in \spn\{u,v\}$ such that $u^\top r = u^\top s$, $v^\top r = v^\top s$. The unit vector $s/\norm{s}$ is uniformly distributed on the unit circle by the $O(d)$-invariance of the probability distribution. 
\end{proof}

\begin{lemma}[Krivine's trick]\label{lem:krivines_trick}
	Let $u_1,\dots,u_m$,$v_1,\dots,v_n\in S^{m+n-1}$ be given. Then there are $u_1^\prime,\dots,u_m^\prime$,$v_1^\prime,\dots,v_n^\prime\in S^{m+n-1}$ so that
	\begin{equation}
		\mathbb{E}[\sgn((u_i^\prime)^\top r)\sgn((v_j^\prime)^\top r)] = \beta u_i^\top v_j,
	\end{equation}		
	with $\beta = \frac{2}{\pi} \ln (1+\sqrt{2}).$
\end{lemma}

For the proof of \ref{lem:krivines_trick} we need to use the tensor product. \textit{We have already seen the tensor product for complex vector spaces. Now we give an alternative definition for the $\mathbb{R}^n$.} The $\mathbb{R}^n$ is an $n$-dimensional Euclidean space with inner product \sclr{\cdot}{\cdot} and orthonormal basis $e_1,\dots,e_n$.
Then the \emph{k-th tensor product of $\mathbb{R}^n$} is denoted by $(\mathbb{R}^n)^{\tensor k}$ and it is a Euclidean  vector space of dimension $n^k$ with orthonormal basis $e_{i_1}\tensor e_{i_2} \tensor \cdots \tensor e_{i_k}$, $i_j\in\{1,\dots,n\}$. In particular:
\begin{equation}
	(e_{i_1}\tensor e_{i_2} \tensor \cdots \tensor e_{i_k})^\top (e_{j_1}\tensor e_{j_2} \tensor \cdots \tensor e_{j_k}) = \prod_{r=1}^k e_{i_r}^\top e_{j_r}
\end{equation}
and for $v\in\mathbb{R}^n$ with $v=v_1e_1+\cdots +v_ne_n$ we define $v^{\tensor k} \in (\mathbb{R}^n)^{\tensor k}$ by 
\begin{equation}
	v^{\tensor k} = (v_1e_1 + \cdots + v_ne_n) \tensor \cdots \tensor (v_1e_1 + \cdots + v_ne_n) = \sum_{i_1,\dots,i_k} v_{i_1}\cdots v_{i_k} e_{i_1}\tensor\cdots\tensor e_{i_k}.
\end{equation}

\begin{proof}
	Hier kommt ein krasser Beweis.
\end{proof}