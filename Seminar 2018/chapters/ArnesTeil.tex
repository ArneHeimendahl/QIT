\subsection{Local Correlation matrices}
So far, we have been dealing with quite specific strategies. The idea is to generalize the concept of strategies into mathematical objects. Suppose the referee sends an element $ s \in \mathcal{S} $ to Alice, respectively $ t \in\mathcal{T} $ to Bob. Suppose Alice and Bob answer according to a deterministic strategy. We will interpret their answers as vectors $ a \in [-1,1]^{\mathcal{S}}, b \in [-1,1]^{\mathcal{T}} $.  Their common answer is the product $ a_sb_t $. So, their strategy can be uniquely described by a  matrix $ ab^\top $.  Instead of playing a deterministic strategy they could answer in a probabilistic way, meaning that given the input $ s \in \mathcal{S} $, respectively $ t \in \mathcal{T} $ their answers are determined by random variables $ X_s $ and $ Y_t $. Then, their expected common answer is $ \mathbb{E}[X_sY_t] $. In the following definition we define the set of all such matrices encoding the common strategy of Alice and Bob.

\begin{dfn}
	Let $ (X_i)_{1 \le i \le m } $ and $ (Y_j)_{1 \le j \le n} $ be families of random variables on a common probability space such that $ \vert X_i \vert, \vert Y_j \vert \le 1 $ almost surely. Then $ A=(a_{ij}) $ is the corresponding {\itshape classical (or local) correlation matrix} if 
	\begin{align*}
		a_{ij} = \mathbb{E}[X_iY_j]
	\end{align*}
	for all $ 1 \le i \le m, 1 \le j \le n $.
\end{dfn}
As we will see in the sequel, the set of $ m \times n $ correlation matrices is a polytope, denoted by $ \LC_{m,n} $.

As we will see in the following two lemmata, both sets can be described in a more useful way. 

\begin{lemma}\label{LemLC}
	An alternative description of $ \LC_{m,n} $ is given by 
	\begin{align}\label{EqLC}
		\LC_{m,n} = \textup{conv} \{  \xi\eta^T \, | \, \xi \in \{-1,1\}^m, \eta \in \{-1,1\}^n     \}.
	\end{align}
\end{lemma}
\begin{proof}
	Let us denote the right hand side of \ref{EqLC} by $ M $ and let $ \xi\eta^T \in M $ with $ \xi \in \{-1,1\}^m, \eta \in \{-1,1\}^n $. Clearly $ \xi_i, \eta_j \in \{-1,1\} $ define constant $ \mathbb{R} $-valued random variables satisfying $ \vert \xi_i \vert, \, \vert \eta_j \vert \le 1 $. Hence, it suffices to show that $ \LC_{m,n} $ is convex since it contains the vertices of $ M $. Therefore, consider two classical correlation matrices $ a_{ij}^{(k)} = \mathbb{E}[X_i^{(k)}Y_{j}^{(k)}] $ for $ k \in \{0,1\} $ which are defined on a common probability space such that $ \modul{X_i}^{(k)},\modul{Y}_j^{(k)} \le 1 $ We have to show that there exists random variables $ (X_i),(Y_j) $ with $ \norm{X_i},\norm{Y_j} \le 1 $ almost surely such that
	\begin{align}
		\beta a_{ij}^{(0)}+ (1-\beta)a_{ij}^{(1)} = \mathbb{E}[X_iY_j]
	\end{align}
	for all $ \beta \in [0,1] $.
	Let $ \alpha $ be a Bernoulli random variable, i.e. $ \mathbb{P}(\alpha = 0) = \beta $, $ \mathbb{P}(\alpha = 1) = 1 - \beta$ and set $ X_i = X_i^{(\alpha)}, Y_j = Y_j^{(\alpha)} $.
	Then 
	\begin{align*}
		\mathbb{E}[X_iY_j] &= \mathbb{E}[X_i^{(\alpha)}Y_j^{(\alpha)}  \mathds{1}_{ \{\alpha = 0\}}] + \mathbb{E}[X_i^{(\alpha)}Y_j^{(1)}]\mathds{1}_{\{\alpha = 1\}}] \\
		&= \beta \mathbb{E}[X_i^{(0)}Y_j^{(0)} ] + (1-\beta) \mathbb{E}[X_i^{(1)}Y_j^{(1)}],
	\end{align*} 
	which proofs that $ \LC_{m,n} $ is convex.
	
	
	
	For the other inclusion, let $ (a_{ij}) \in \LC_{m,n} $, i.e. $ a_{ij} = \mathbb{E}[X_iY_j] $ for $ \mathbb{R} $-valued random variables $ (X_i),(Y_j) $, defined on a common probability space $ \Omega $ with $ \modul{X_i},\modul{Y_j} \le 1 $ almost surely. 
	We will use the characterization of the $ d-$dimensional cube by its vertices, that is $ [-1,1]^d = \textup{conv} \{\xi \, | \, \xi \in \{-1,1 \}^d \}$ (this can be proved by induction). 
	If we define the random variables $ X= (X_1,...,X_m) $ and $ Y= (Y_1,...,Y_n) $ they satisfy $ X \in [-1,1]^m, \, Y \in [-1,1]^n $ almost surely. Using the characterization of the hypercube we can define random variables $ \lambda_{\xi}^{(X)}: \Omega^m \to [0,1] $ such that 
	\begin{align*}
		X(\omega) = \sum_{\xi \in \{-1,1\}^m}\lambda_{\xi}^{(X)}(\omega)\xi
	\end{align*} 
	almost surely 
	and $ \sum_{\xi \in \{-1,1\}^m}\lambda_{\xi}^{(X)}(\omega) = 1  $. We can deduce that 
	$X_i =  \sum_{\xi \in \{-1,1\}^m}\lambda_{\xi}^{(X)}\xi_i $ almost surely.
	If we proceed analogously for $ Y $ we obtain
	\begin{align*}
		a_{ij} = \mathbb{E}[X_iY_j] &= \mathbb{E} \big [  (\sum_{\xi \in \{-1,1\}^m}\lambda_{\xi}^{(X)}\xi_i ) (\sum_{\eta \in \{-1,1\}^n}\lambda_{\eta}^{(Y)}\eta_j ) \big ]   \\
		&= \sum_{\xi \in \{-1,1\}^m, \eta \in \{-1,1\}^n} \mathbb{E}\big [\lambda_{\xi}^{(X)}\lambda_{\eta}^{(Y)} \big ] \xi_i \eta_j  \\
		&= \big (\sum_{\xi \in \{-1,1\}^m, \eta \in \{-1,1\}^n}\mathbb{E} [\lambda_{\xi}^{(X)} ]\mathbb{E}[\lambda_{\eta}^{(Y)}]\big )\xi_i\eta_j
	\end{align*}
	where we used that $ \lambda_{\xi}^{(X)} $ and $ \lambda_{\eta}^{(Y)} $ are independent.
	Since $
		\sum_{\xi \in \{-1,1\}^m, \eta \in \{-1,1\}^n}\mathbb{E} [\lambda_{\xi}^{(X)} ]\mathbb{E}[\lambda_{\eta}^{(Y)}] = 1 $ it follows that $ (a_{ij}) \in  \textup{conv} \{  \xi\eta^T \, | \, \xi \in \{-1,1\}^m, \eta \in \{-1,1\}^n     \}$
 which finishes the proof.
\end{proof}
Now we are able to count the vertices of $ \LC_{m,n} $. Observing that $ \xi \eta^T = \tilde{\xi} \tilde{\eta}^T $ if and only if $ \xi = \tilde{\xi} $ and $ \eta = \tilde{\eta} $ or $ \xi = -\tilde{\xi} $ and $ \eta = -\tilde{\eta} $ it follows that we have $ 2^{n+m}/2 = 2^{n+m-1} $ different matrices $ \xi \eta $, hence $ \LC_{m,n} $ has $ 2^{n+m-1} $ vertices. To analyze the facial structure of $ \LC_{m,n} $ is rather complicated. 
However, we will do it later on for $ n=m=2 $ and compare it to $ \QC_{m,n} $.

\subsection{Quantum correlation matrices}

\begin{dfn}
	Let $ (X_i)_{1 \le i \le m } $ and $ (Y_j)_{1 \le j \le n} $ be self-adjoint operators on $ \mathbb{C}^{d_1} $, respectively $ \mathbb{C}^{d_2} $ for some positive integers $ d_1,d_2 $, satisfying $ \norm{X_i}_{\infty}, \norm{Y_j}_{\infty} \le 1 $. $ A = (a_{ij}) $ is called {\itshape quantum correlation matrix} if there exists a state \textbf{Introduce a symbol zo define operators form one space to another} $ \rho \in D(\mathbb{C} \otimes \mathbb{C})$ such that 
	\begin{align}\label{QCaij}
	a_{ij} = \trace{\rho (X_i \otimes Y_j)}.
	\end{align}
\end{dfn}
We will write $ \QC_{m,n} $ for the set of all $ m \times n $ quantum correlation matrices.

With regard to quantum information theory it is interesting to analyze the geometry of $ \LC_{m,n} $ and $ \QC_{m,n} $. 


In the following, we will proof a similar result for $ \QC_{m,n} $ that is: 
\begin{lemma}\label{LemQC}
	\begin{align*}\label{EqQC}
		QC_{m,n} = \{ (\langle x_i,y_j \rangle)_{1 \le 1 \le m, 1 \le j \le n} \,| \, x_i,y_j \in \mathbb{R}^{ \min \{m,n \} }, \vert x_i  \vert \le 1, \vert y_j \vert \le 1  \},
	\end{align*}
	where $ \langle \cdot , \cdot \rangle $ denotes the standard scalar product. 
\end{lemma}
In order to proof this we have to review some definitions and introduce a special class of matrices, namely the {\itshape Pauli matrices}.

For the first inclusion we review the definition of an inner product. The basic idea is to define an inner product via the  definition of the $ a_{ij} $ in \ref{QCaij}.
Let $ V $ and $ W $ be two vector spaces and $ k  $ a field. A {\itshape bilinear form} is a map $ \beta: \, V \times W \to k $ that is linear in both variables, that is 
\begin{enumerate}
	\item $\beta(v_1+v_2,w) = \beta(v_1,w) + \beta(v_2,w)  $
	\item  $\beta(\lambda v,w)= \lambda \beta(v,w) $
	\item $ \beta(v,w_1+w_2) = \beta(v,w_1)+ \beta(v,w_2) $
	\item $ \beta(v,\lambda w) = \lambda\beta(v,w) $
\end{enumerate}
for all $ v,v_1,v_2 \in V, \, w,w_1,w_2 \in W, \, \lambda \in k $. 
If $ V = W $, we call $ \beta $ {\itshape symmetric} if $ \beta(v,w) = \beta(w,v) $, {\itshape positive semidefinite} if 
$ \beta(v,v) \ge 0 $ and {\itshape positive definite} if $ \beta $ is positive semidefinite and $ \beta(v,v)= 0 $ implies that $ v = 0 $. 
If $ \beta: \, V \times V \to k $ is a symmetric positive definite bilinear form it is called an {\itshape inner product} and usually denoted by $ \langle \cdot \, , \, \cdot \rangle $.
Again, we will write $ M $ for the right hand side of equation \ref{EqQC}.
\begin{proof}[Proof of $ \QC_{m,n} \subset M $]
	Let $ (a_{ij}) \in \QC_{m,n} $. Then there is a sate $ \rho $ on a Hilbert space $ \mathcal{H} = \mathbb{C}^{d_1} \otimes\mathbb{C}^{d_2} $ and Hermitian operators $ (X_i)_{1 \ge m}, \, (Y_j)_{1 \ge n} $ on $ \mathbb{C}^{d_1} $, respectively $ \mathbb{C}^{d_2} $ satisfying $ \norm{X_i}_{\infty}, \norm{Y_j}_{\infty} \le 1 $ such that 
	$ a_{ij} = \trace{\rho X_i \otimes Y_j} $.
	We define a positive semidefinite symmetric bilinear form on the space of Hermitian operators on $ \mathcal{H} $ by 
	$ \beta: \mathcal{H} \times \mathcal{H} \to \mathbb{R} $ where $ \beta(S,T) =\textup{Re}( \trace{\rho ST}) $.
	We have to check that it indeed satisfies the mentioned properties. 
	Obviously, $ \beta $ is homogeneous in both variables due to the fact that the trace and the real part of a complex number are linear functions and thus homogeneous. We will show additivity for the first variable, the result follows analogously for the second one. It holds
	\begin{align*}
		\beta(S_1+S_2,T) = \textup{Re}(\trace{\rho(S_1+S_2)T}) &= \textup{Re}(\trace{\rho S_1T}) + \textup{Re}(\trace{\rho S_2T}) \\
		&= \beta(S_1,T)+ \beta(S_2+T).
	\end{align*}
	Symmetry follows form 
	\begin{align*}
		\beta(S,T)&= \textup{Re}\trace{\rho ST} =\textup{Re}\trace{(\rho ST)^*}  = \textup{Re \trace{T^*S^* \rho^*}}  \\
		 &=\textup{Re} \trace{\rho^* T^*S^*} = \textup{Re} \trace{\rho T S} = \beta(T,S),
	\end{align*}
	\textbf{Perhaps explanation}
	
	Moreover, since $ S^*S $ is a positive semidefinite operator for all complex operators $ S $ we obtain $ \beta(S,S) = \textup{Re}\trace{\rho SS} = \textup{Re} \trace{\rho S^*S} \ge 0 $ and $ \beta $ is positive semidefinite. 
	
	Following the steps in appendix \textbf{XXX} we can factorize the kernel and transform $ \beta $ to an inner product on the vector space of self-adjoint operators modulo the kernel of $ \beta $. 
	Equipped with an inner product, we can regard $ \textup{B}^{sa}(\mathcal{H}) $ \textbf{Introduce notation} as a real Euclidean space. 
	We immediately get that $ a_{ij} =\trace{\rho X_i Y_j}=  \beta(X_i \otimes I,I \otimes Y_j) $. In order to show that the norms of our vectors are bounded by one we have to show
	$ \beta(X \otimes I, X \otimes I), \beta(I \otimes Y, I \otimes Y) \le 1$ for all $ X \in \{X_1,...,Y_m \} $, $ Y \in \{Y_1,...,Y_n \} $. 
	Therefore, let $ d = d_1d_2 $ and $ \rho = \vert \phi \rangle \langle \phi \vert $ for $ \vert \phi \rangle = \sum_{i}^{d}\lambda_i \xi_i \otimes \eta_i $, where $ \{ \xi_1,...,x_d \} \subset \mathbb{C}^{d_1}$ and $ \{  \eta_1,...,\eta_d\} \subset \mathbb{C}^{d_2}$ are orthonormal sets, i.e. $ \braket{\xi_i \vert \xi_j}, \braket{\eta_i \vert \eta_j} = 0 $ for $ i \neq j $, and $ \sum_{i=1}^d \lambda_i^2 = 1 $ This decomposition is called {\itshape Schmidt decomposition} and a consequence of the singular value decomposition. Writing $ \rho $ in this form we get 
	\begin{align*}
		\beta(X \otimes I, X \otimes I) &= \textup{Re} \trace{\rho X^2 \otimes I}=  \sum_{i=1}^d \lambda_i^2 \trace{(\ket{\xi_i} \bra{\xi_i} \otimes \ket{\eta_i}\bra{\eta_i})(X^2 \otimes I) } \\
		&= \sum_{i=1}^{d}\lambda_i^2 \trace{(\ket{\xi_i} \bra{\xi_i}X^2)}\trace{(\ket{\eta_i}\bra{\eta_i})} =  \sum_{i=1}^{d}\lambda_i^2 \trace{(\ket{\xi_i} \bra{\xi_i}X^2)}.
	\end{align*}
	In order to get the desired result we have to show that $ \trace{(\ket{\xi_i} \bra{\xi_i}X^2)} = \trace{(X^2\ket{\xi_i} \bra{\xi_i})} \le 1 $. 
	Note that $1 \ge \norm{X}_{\infty} := \sup_{\modul{y}\le 1} \modul{Xy} $ implies that $ \modul{X^2\ket{\xi_i}} \le 1 $. So the problem can be reduced to $ \vert \trace{uv^*}\vert^2 \le 1 $ for complex vectors $ u,v $ with $ \modul{u},\, \modul{v} \le 1 $. But this holds since due to the Cauchy-Schwarz inequality 
	$ \vert\trace{uv^*}\vert^2 = \vert\sum u_i \bar{v_i}\vert^2 \le \modul{u}^2\modul{v}^2 \le 1 $.
	
	If we now identify the operators $ X_i \otimes I $ and $  I \otimes Y_j$ with vectors
	$ (x_i) $ and $ (y_j) $ we have found vectors that almost satisfy the desired properties but they do not have the right dimension and we do not consider the standard scalar product yet. Without loss of generality let $ m \le n $. To obtain the required dimension we will project $ (y_j) $ orthogonally onto $ \textup{span}\{ x_1,...,x_m \} $.
	Let $ \{a_1,...,a_r\} $ be an orthonormal basis of $ \textup{span} \{ x_1,...,x_m \} $ with respect to $ \beta $. 
	The orthogonal projection of $ y_j $ is $ \pi(y):= \sum_{i=1}^{r}\beta(a_i,y_j)a_i $ and fulfills
	$ \beta(x_i,y_j) = \beta(x_i,\pi(y_j)) $. \textbf{perhaps explanation but standard calculation}
	Let $ x_i $ and $ \pi(y_j) $ admit the descriptions 
	$ x_i = \sum_{k=1}^{r}\alpha_k^{(i)}a_k$ and $  \pi(y_j) = \sum_{k=1}^r \gamma_k^{(j)} a_k$ for $ \alpha^{(i)}, \gamma^{(j)} \in \mathbb{R}^r $. Then
	\begin{align*}
		a_{ij}= \beta(x,y)= \beta(x,\pi(y)) = \sum_{1 \le k,l \le r} \alpha_k^{(i)} \gamma_l^{(j)} \beta(a_k,a_l) = \sum_{k=1}^{r}\alpha_k^{(i)}\gamma_k^{(j)} = \langle \alpha^{(i)}a, \gamma^{(j)} \rangle.
	\end{align*}
	Moreover, since $ \modul{x_i}, \, \modul{y_j}= \modul{\pi(y_j)} \le 1 $ we get $ \langle \alpha^{(i)}, \alpha^{(i)} \rangle, \, \langle \gamma^{(j)}, \gamma^{(j)} \rangle \le 1$ and thus the vectors $ (\alpha^{(i)}) $ and$ (\gamma^{(j)}) $ all required properties. 
	Additionally, we also proved that we can take an even lower dimension for our vectors, precisely 
	$ \min \{ \dim( \textup{span} \{ (x_i) \}),$ $ \dim(\textup{span} \{ (y_j) \} )\} $.
	 
	 
	
\end{proof}
\begin{proof}[Proof of $ \LC_{m,n} \supset M $]
	\textbf{Following}
\end{proof}
We can easily see that $ \QC_{m,n} $ is convex. Consider $ (a_{ij}), (\bar{a}_{ij}) \in \QC_{m,n} $ with $ a_{ij} = \langle x_i, y_j \rangle $ and $ \bar{a}_{ij} = \langle \bar{x}_i, \bar{y}_j \rangle $ for $ x_{i},y_j, \bar{x}_i,\bar{y}_j \in \mathbb{R}^{\min \{m,n\}} $ such that $ \modul{x_i},\modul{y_j}, \modul{\bar{x}_i}, \modul{\bar{y}_j} \le 1 $.
For $ \lambda \in [0,1] $ we define vectors $\tilde{x}_i:= (\sqrt{\lambda}x_i,\sqrt{1-\lambda}\bar{x_i}), \, \tilde{y}_j:= (\sqrt{\lambda}y_j, \sqrt{1-\lambda}\bar{y}_j) $ and due to  $ \modul{\tilde{x}_i} \le \lambda \modul{(x_i,0)} + (1-\lambda)\modul{(0,\bar{x}_i)} \le 1 $ they are unit vectors. Moreover, $ \langle \tilde{x}_i, \tilde{y}_j \rangle = \lambda \langle x_i,y_j \rangle + (1-\lambda) \langle \tilde{x_i},\tilde{y}_j \rangle$. If we proceed in the same fashion as in the proof of lemma \ref{LemQC} we obtain vectors 
$ \alpha^{(i)},\gamma^{(j)} $ that satisfy $= \langle \alpha^{i},\gamma^{j} \rangle = \langle \tilde{x} _i, \tilde{y}_j$ and have dimension smaller or equal to $ \min \{m,n\} $.

Using these both descriptions we can derive some relations between the two sets. 
Let $ \xi\eta^T $ be a vertex of $ \LC_{m,n} $. If we just choose $ x_i = \xi_i \ket{0}$ and $ y_j = \eta_j\ket{0} $ we immediately see that $ \xi_i\eta_j = \langle x_i, y_j \rangle $. Hence, $ \xi\eta^T \in \QC_{m,n} $ and combined with the convexity of $ \QC_{m,n} $ we get $ \LC_{m,n} \subset \QC_{m,n} $.

However, the inclusion is strict in general. Let us consider $ n=m=2 $. 
For $ \LC_{2,2} $ we obtain 
\begin{align*}
	\LC_{2,2}= \textup{conv} \{ \pm \begin{pmatrix}
	1 & 1 \\
	1 & 1
	\end{pmatrix} , \, \pm \begin{pmatrix}
	-1 & -1 \\
	1 & 1
	\end{pmatrix} , \, \pm \begin{pmatrix}
	-1 & 1 \\
	-1 & 1
	\end{pmatrix}, \, \pm \begin{pmatrix}
	-1 & 1 \\
	1 & -1
	\end{pmatrix}  \}.
\end{align*}
We can easily see that $ \sigma(\begin{pmatrix}
-1 & 1 \\ 1 & 1 
\end{pmatrix}) \notin \LC_{2,2} $ for $ \sigma \in \Sigma_4 $ where for $ A = (a_{ij}) $ we define $ \sigma (A)  $ by $ (\sigma( A))_{ij}:= a_{\sigma(1)\sigma(j)} $.
We claim that
\begin{equation}\label{facetsLC}
	\QC_{2,2} = \{ A \in \mathbb{R}^{2 \times 2} \, | \, -1 \le  \trace{AM} \le 1 \textup{ for all } M \in \mathcal{K} \},
\end{equation}
where $ \mathcal{K}  = \{ \frac{1}{2}\sigma(\begin{pmatrix}
-1 & 1 \\
1 & 1
\end{pmatrix}), \sigma(\begin{pmatrix}
1 & 0 \\
0 & 0
\end{pmatrix}) \, | \,  \sigma \in \{  (1 \, \, 2), (1 \, \, 3), (1 \, \, 4) \} \} $.
The crucial observation is to note that $ \LC_{2,2} $ is affinely isomorphic to the cross polytope scaled  by two, i.e. 
$ \LC_{2,2} \cong 2\textup{CP}_4 := \textup{conv} \{  \pm e_i \, | \, i = 1,...,4 \} $, where $ e_i $ are the vectors of the standard basis of $ \mathbb{R}^4 $. For example, this can be seen by interpreting the vertices of $ \LC_{2,2} $ as elements of $ \mathbb{R}^4 $ and then apply the linear transformation given by the matrix 
\begin{align*}
	\frac{1}{2}\begin{pmatrix}
		-1 & -1 & 1 & 1 \\
		-1 & 1 & -1 & 1 \\
		-1 & 1 & 1 & -1 \\
		1 & 1 & 1 & 1 
	\end{pmatrix}.
\end{align*}
An easy analysis shows that $ (\textup{CP}_4)^{o}  = [-1,1]^4$ \textbf{maybe introduce polar dual}, hence the face lattice of $ \textup{CP}_4 $ is isomorphic to the inverse \textbf{look up the correct name} lattice of the hypercube which implies that the number of facets of $ \textup{CP}_4 $ coincides with the number of vertices of $ [-1,1]^4 $ which is $ 2^4 $. Due to $  \LC_{2,2} \cong 2\textup{CP}_4$ their face lattices of $ \LC_{2,2} $ and $ \textup{CP}_4 $ are isomorphic, so $ \LC_{2,2} $ has $ 2^4 $ facets as well. 
Since all constraints in \ref{facetsLC} clearly define non-empty faces of $ \LC_{2,2} $, it suffices to show that the characterization is a non-redundant hyperplane description of $ \LC_{2,2} $, implying that all constraints define facets of $ \LC_{2,2} $. But this is indeed true since if we omit for example the constraint 
$ 1/2 \trace{AM} \le 1 $ for $ M = \{ \begin{pmatrix}
1 & 0 \\ 0 & 0
\end{pmatrix} \} $, respectively $ M = \begin{pmatrix}
-1 & 1 \\ 1 & 1 
\end{pmatrix} $
the matrices $ \begin{pmatrix}
2 & 0 \\ 0 & 0 
\end{pmatrix} $, respectively $ \begin{pmatrix}
-1 & 1 \\ 1 & 1
\end{pmatrix} $ 
satisfy all other constraints. 
Eventually, we have all information to show that $ \LC_{2,2} $ is a proper subset of $ \QC_{2,2} $. 
Assume we want to maximize in the direction of the facet induced by $ 1/2M = \begin{pmatrix}
1 & 1 \\ 1 & -1 
\end{pmatrix} $. Since $ \max \{  1/2 \trace{AM}\, | \, A \in \LC_{2,2} \} = 1 $ it suffices to show that there is $ A \in \QC_{m,n} $ achieving a better value. For $ A \in \QC_{2,2} $ we obtain, by lemma \ref{LemQC}, Cauchy-Schwarz and $ \modul{y_i}\le 1 $,
\begin{align*}
	\trace{AM}&= \langle x_1,y_1 \rangle + \langle x_1,y_2 \rangle + \langle x_2,y_1 \rangle - \langle x_2,y_2 \rangle   \\
	&= \langle x_1+x_2,y_1 \rangle + \langle x_1-x_2,y_2 \rangle  \le \modul{x_1+x_2}\modul{y_1} + \modul{x_1-x_2}\modul{y_2}  \\
	&\le \modul{x_1+x_2} + \modul{x_1-x_2}.
\end{align*} 
Observing that for $ \modul{x_1},\modul{x_2}\le  1 $
\begin{align*}
	&(\modul{x_1+x_2} + \modul{x_1-x_2})^2 \\
	&= \langle x_1+x_2,x_1+x_2\rangle  +\sqrt{\langle x_1+x_2,x_1+x_2\rangle  \langle x_1-x_2,x_1-x_2\rangle} +  \langle x_1-x_2,x_1-x_2\rangle  \\
	&\le 2\modul{x_1}^2+2\modul{x_2}^2 + \sqrt{\modul{x_1}^4+2\modul{x_1}^2\modul{x_2}^2-4\langle x_1,x_2\rangle^2+\modul{x_2}^4}  \\
	&\le  2\modul{x_1}^2+2\modul{x_2}^2 + \sqrt{4(\modul{x_1}^2+\modul{x_2}^2)^2}  \\
	&= 4(\modul{x_1}^2+\modul{x_2}^2)
\end{align*}
we can give a precise upper bound for $ \trace{AM} $ by 
\begin{align*}
	\modul{x_1+x_2} + \modul{x_1-x_2} \le 2\sqrt{\modul{x_1}^2+\modul{x_2}^2} \le 2 \sqrt{2}.
\end{align*}
Thus, we just have to find a matrix that satisfies this bound. A possible choice is induced by the vectors $ x_1 = x_2 = \frac{1}{\sqrt{2}}(1,1) $ and $ y_1 = y_2 =(1,0) $, that is $ A = \frac{1}{\sqrt{2}}\begin{pmatrix}
1 & 1 \\ 1 & 1 
\end{pmatrix} $ yielding the value $ \trace{AM} = 2 \sqrt{2} $.