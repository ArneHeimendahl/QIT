\subsection{Local and Quantum correlation matrices}
\begin{dfn}
	Let $ (X_i)_{1 \le i \le m } $ and $ (Y_j)_{1 \le j \le n} $ be families of random variables on a common probability space such that $ \vert X_i \vert, \vert Y_j \vert \le 1 $ almost surely \textbf{define the norm}. Then $ A=(a_{ij}) $ is the corresponding {\itshape classical (or local) correlation matrix} if 
	\begin{align*}
		a_{ij} = \mathbb{E}[X_iY_j]
	\end{align*}
	for all $ 1 \le i \le m, 1 \le j \le n $.
\end{dfn}
As we will see in the sequel, the set of $ m \times n $ correlation matrices is a polytope, denoted by $ \LC_{m,n} $.

\begin{dfn}
	Let $ (X_i)_{1 \le i \le m } $ and $ (Y_j)_{1 \le j \le n} $ be self-adjoint operators on $ \mathbb{C}^{d_1} $, respectively $ \mathbb{C}^{d_2} $ for some positive integers $ d_1,d_2 $, satisfying $ \norm{X_i}_{\infty}, \norm{Y_j}_{\infty} \le 1 $. $ A = (a_{ij}) $ is called {\itshape quantum correlation matrix} if there exists a state \textbf{Introduce a symbol zo define operators form one space to another} $ \rho \in D(\mathbb{C} \otimes \mathbb{C})$ such that 
	\begin{align}\label{QCaij}
		a_{ij} = \trace{\rho (X_i \otimes Y_j)}.
	\end{align}
\end{dfn}
We will write $ \QC_{m,n} $ for the set of all $ m \times n $ quantum cor relation matrices.
With regard to quantum information theory it is interesting to analyze the geometry of $ \LC_{m,n} $ and $ \QC_{m,n} $. As we will see in the following two lemmata, both sets have rather simple descriptions. 

\begin{lemma}\label{LemLC}
	An alternative description of $ \LC_{m,n} $ is given by 
	\begin{align}\label{EqLC}
		\LC_{m,n} = \textup{conv} \{  \xi\eta^T \, | \, \xi \in \{-1,1\}^m, \eta \in \{-1,1\}^n     \}.
	\end{align}
\end{lemma}
\begin{proof}
	Let us denote the right hand side of \ref{EqLC} by $ M $ and let $ \xi\eta^T \in M $ with $ \xi \in \{-1,1\}^m, \eta \in \{-1,1\}^n $. Clearly $ \xi_i, \eta_j \in \{-1,1\} $ define constant $ \mathbb{R} $-valued random variables satisfying $ \vert X_i \vert, \, \vert Y_j \vert \le 1 $. Hence, it suffices to show that $ \LC_{m,n} $ is convex since it contains the vertices of $ M $. Therefore, consider to classical correlation matrices $ a_{ij}^{(k)} = \mathbb{E}[X_i^{(k)}Y_{j}^{(k)}] $ for $ k \in \{0,1\} $ which are defined on a common probability space and whose absolute value is smaller than one almost surely. We have to show that there exists random variables $ (X_i),(Y_j) $ with $ \norm{X_i},\norm{Y_j} \le 1 $ almost surely such that
	\begin{align}
		\beta a_{ij}^{(0)}+ (1-\beta)a_{ij}^{(1)} = \mathbb{E}[X_iY_j]
	\end{align}
	for all $ \beta \in [0,1] $.
	Let $ \alpha $ be a Bernoulli random variable, i.e. $ \mathbb{P}(\alpha = 0) = \beta $, $ \mathbb{P}(\alpha = 1) = 1 - \beta$ and set $ X_i = X_i^{(\alpha)}, Y_j = Y_j^{(\alpha)} $.
	Then 
	\begin{align*}
		\mathbb{E}[X_iY_j] &= \mathbb{E}[X_i^{(\alpha)}Y_j^{(\alpha)}  \mathds{1}_{ \{\alpha = 0\}}] + \mathbb{E}[X_i^{(\alpha)}Y_j^{(1)}]\mathds{1}_{\{\alpha = 1\}}] \\
		&= \beta \mathbb{E}[X_i^{(0)}Y_j^{(0)} ] + (1-\beta) \mathbb{E}[X_i^{(1)}Y_j^{(1)}],
	\end{align*} 
	which proofs that $ \LC_{m,n} $ is convex.
	
	
	
	For the other inclusion, let $ (a_{ij}) \in \LC_{m,n} $, i.e. $ a_{ij} = \mathbb{E}[X_iY_j] $ for $ \mathbb{R} $-valued random variables $ (X_i),(Y_j) $, defined on a common probability space $ \Omega $ with $ \modul{X_i},\modul{Y_j} \le 1 $ almost surely. 
	We will use the characterization of the $ d-$dimensional cube by its vertices, that is $ [-1,1]^d = \textup{conv} \{\xi \, | \, \xi \in \{-1,1 \}^d \}$ (proof by induction). So, for 
	If we define the random variables $ X= (X_1,...,X_m) $ and $ Y= (Y_1,...,Y_m) $ they are essentially functionals 
	$ X: \Omega^m \mapsto [-1,1]^m $ (up to a null set) {\textbf{refomulate }}. Using the characterization of the hypercube we can define random variables $ \lambda_{\xi}^{(X)}: \Omega^m \to [0,1] $ for $ \Omega \in \{-1,1\}^m $ such that 
	\begin{align*}
		X(\omega) = \sum_{\xi \in \{-1,1\}^m}\lambda_{\xi}^{(X)}(\omega)\xi
	\end{align*} 
	and $ \sum_{\xi \in \{-1,1\}^m}\lambda_{\xi}^{(X)}(\omega) = 1  $. 
	If we proceed analogously for $ Y $ we obtain
	\begin{align*}
		a_{ij} = \mathbb{E}[X_iY_j] &= \mathbb{E} \big [  (\sum_{\xi \in \{-1,1\}^m}\lambda_{\xi}^{(X)}\xi_i ) (\sum_{\eta \in \{-1,1\}^n}\lambda_{\eta}^{(Y)}\eta_j ) \big ]   \\
		&= \sum_{\xi \in \{-1,1\}^m, \eta \in \{-1,1\}^n} \mathbb{E}\big [\lambda_{\xi}^{(X)}\lambda_{\eta}^{(Y)} \big ] \xi_i \eta_j  \\
		&=\Big ( \mathbb{E}\big [\sum_{\eta \in \{-1,1\}^m}\lambda_{\xi}^{(X)} \big ] \mathbb{E}\big [\sum_{\eta \in \{-1,1\}^n}\lambda_{\eta}^{(Y)} \big ] \Big ) \xi_i \eta_j  \\
		&= \xi_i\eta_j,
	\end{align*}
	where we used that $ \lambda_{\xi}^{(X)} $ and $ \lambda_{\eta}^{(Y)} $ are independent and sum up to one. 
	Thus, $ \{ a_{ij}\} \in M $ which finishes the proof. 
\end{proof}
Now we can easily count the vertices of $ \LC_{m,n} $. Observing that $ \xi \eta^T = \tilde{\xi} \tilde{\eta}^T $ if and only if $ \xi = \tilde{\xi} $ and $ \eta = \tilde{\eta} $ or $ \xi = -\tilde{\xi} $ and $ \eta = -\tilde{\eta} $ it follows that we have $ 2^{n+m}/2 = 2^{n+m-1} $ different matrices $ \xi \eta $, hence $ \LC_{m,n} $ has $ 2^{n+m-1} $ vertices. To analyze the facial structure of $ \LC_{m,n} $ is rather complicated. 
However, we will do it later on for $ n=m=2 $ and compare it to $ \QC_{m,n} $.

In the following, we will proof a similar description for $ \QC_{m,n} $ that is: 
\begin{lemma}\label{LemQC}
	\begin{align*}\label{EqQC}
		QC_{m,n} = \{ (\langle x_i,y_j \rangle)_{1 \le 1 \le m, 1 \le j \le n} \,| \, x_i,y_j \in \mathbb{R}^{ \min \{m,n \} }, \vert x_i  \vert \le 1, \vert y_j \vert \le 1  \},
	\end{align*}
	where $ \langle \cdot , \cdot \rangle $ denotes the standard scalar product. 
\end{lemma}
In order to proof this we have to review some definitions and introduce a special class of matrices, namely the {\itshape Pauli matrices}.

For the first inclusion we review the definition of an inner product. The basic idea is to define an inner product via the  definition of the $ a_{ij} $ in \ref{QCaij}.
Let $ V $ and $ W $ be two vector spaces and $ k  $ a field. A {\itshape bilinear form} is a map $ \beta: \, V \times W \to k $ that is linear in both variables, that is 
\begin{enumerate}
	\item $\beta(v_1+v_2,w) = \beta(v_1,w) + \beta(v_2,w)  $
	\item  $\beta(\lambda v,w)= \lambda \beta(v,w) $
	\item $ \beta(v,w_1+w_2) = \beta(v,w_1)+ \beta(v,w_2) $
	\item $ \beta(v,\lambda w) = \lambda\beta(v,w) $
\end{enumerate}
for all $ v,v_1,v_2 \in V, \, w,w_1,w_2 \in W, \, \lambda \in k $. 
If $ V = W $, we call $ \beta $ {\itshape symmetric} if $ \beta(v,w) = \beta(w,v) $, {\itshape positive semidefinite} if 
$ \beta(v,v) \ge 0 $ and {\itshape positive definite} if $ \beta $ is positive semidefinite and $ \beta(v,v)= 0 $ implies that $ v = 0 $. 
If $ \beta: \, V \times V \to k $ is a symmetric positive definite bilinear form it is called an {\itshape inner product} and usually denoted by $ \langle \cdot \, , \, \cdot \rangle $.
Again, we will write $ M $ for the right hand side of equation \ref{EqQC}.
\begin{proof}[Proof of $ \QC_{m,n} \subset M $]
	Let $ (a_{ij}) \in \QC_{m,n} $. Then there is a sate $ \rho $ on a Hilbert space $ \mathcal{H} = \mathbb{C}^{d_1} \otimes\mathbb{C}^{d_2} $ and Hermitian operators $ (X_i)_{1 \ge m}, \, (Y_j)_{1 \ge n} $ on $ \mathbb{C}^{d_1} $, respectively $ \mathbb{C}^{d_2} $ satisfying $ \norm{X_i}_{\infty}, \norm{Y_j}_{\infty} \le 1 $ such that 
	$ a_{ij} = \trace{\rho X_i \otimes Y_j} $.
	We define a positive semidefinite symmetric bilinear form on the space of Hermitian operators on $ \mathcal{H} $ by 
	$ \beta: \mathcal{H} \times \mathcal{H} \to \mathbb{R} $ where $ \beta(S,T) =\textup{Re}( \trace{\rho ST}) $.
	We have to check that it indeed satisfies the mentioned properties. 
	Obviously, $ \beta $ is homogeneous in both variables due to the fact that the trace and the real part of a complex number are linear functions and thus homogeneous. We will show additivity for the first variable, the result follows analogously for the second one. It holds
	\begin{align*}
		\beta(S_1+S_2,T) = \textup{Re}(\trace{\rho(S_1+S_2)T}) &= \textup{Re}(\trace{\rho S_1T}) + \textup{Re}(\trace{\rho S_2T}) \\
		&= \beta(S_1,T)+ \beta(S_2+T).
	\end{align*}
	Symmetry follows form 
	\begin{align*}
		\beta(S,T)&= \textup{Re}\trace{\rho ST} =\textup{Re}\trace{(\rho ST)^*}  = \textup{Re \trace{T^*S^* \rho^*}}  \\
		 &=\textup{Re} \trace{\rho^* T^*S^*} = \textup{Re} \trace{\rho T S} = \beta(T,S),
	\end{align*}
	\textbf{Perhaps explanation}
	
	Moreover, since $ S^*S $ is a positive semidefinite operator for all complex operators $ S $ we obtain $ \beta(S,S) = \textup{Re}\trace{\rho SS} = \textup{Re} \trace{\rho S^*S} \ge 0 $ and $ \beta $ is positive semidefinite. 
	
	Following the steps in appendix \textbf{XXX} we can factorize the kernel and transform $ \beta $ to an inner product on the vector space of self-adjoint operators modulo the kernel of $ \beta $. 
	Equipped with an inner product, we can regard $ \textup{B}^{sa}(\mathcal{H}) $ \textbf{Introduce notation} as a real Euclidean space. 
	We immediately get that $ a_{ij} =\trace{\rho X_i Y_j}=  \beta(X_i \otimes I,I \otimes Y_j) $. In order to show that the norms of our vectors are bounded by one we have to show
	$ \beta(X \otimes I, X \otimes I), \beta(I \otimes Y, I \otimes Y) \le 1$ for all $ X \in \{X_1,...,Y_m \} $, $ Y \in \{Y_1,...,Y_n \} $. 
	Therefore, let $ d = d_1d_2 $ and $ \rho = \vert \phi \rangle \langle \phi \vert $ for $ \vert \phi \rangle = \sum_{i}^{d}\lambda_i \xi_i \otimes \eta_i $, where $ \{ \xi_1,...,x_d \} \subset \mathbb{C}^{d_1}$ and $ \{  \eta_1,...,\eta_d\} \subset \mathbb{C}^{d_2}$ are orthonormal sets, i.e. $ \braket{\xi_i \vert \xi_j}, \braket{\eta_i \vert \eta_j} = 0 $ for $ i \neq j $, and $ \sum_{i=1}^d \lambda_i^2 = 1 $ This decomposition is called {\itshape Schmidt decomposition} and a consequence of the singular value decomposition. Writing $ \rho $ in this form we get 
	\begin{align*}
		\beta(X \otimes I, X \otimes I) &= \textup{Re} \trace{\rho X^2 \otimes I}=  \sum_{i=1}^d \lambda_i^2 \trace{(\ket{\xi_i} \bra{\xi_i} \otimes \ket{\eta_i}\bra{\eta_i})(X^2 \otimes I) } \\
		&= \sum_{i=1}^{d}\lambda_i^2 \trace{(\ket{\xi_i} \bra{\xi_i}X^2)}\trace{(\ket{\eta_i}\bra{\eta_i})} =  \sum_{i=1}^{d}\lambda_i^2 \trace{(\ket{\xi_i} \bra{\xi_i}X^2)}.
	\end{align*}
	In order to get the desired result we have to show that $ \trace{(\ket{\xi_i} \bra{\xi_i}X^2)} = \trace{(X^2\ket{\xi_i} \bra{\xi_i})} \le 1 $. 
	Note that $1 \ge \norm{X}_{\infty} := \sup_{\modul{y}\le 1} \modul{Xy} $ implies that $ \modul{X^2\ket{\xi_i}} \le 1 $. So the problem can be reduced to $ \vert \trace{uv^*}\vert^2 \le 1 $ for complex vectors $ u,v $ with $ \modul{u},\, \modul{v} \le 1 $. But this holds since due to the Cauchy-Schwarz inequality 
	$ \vert\trace{uv^*}\vert^2 = \vert\sum u_i \bar{v_i}\vert^2 \le \modul{u}^2\modul{v}^2 \le 1 $.
	
	If we now identify the operators $ X_i \otimes I $ and $  I \otimes Y_j$ with vectors
	$ (x_i) $ and $ (y_j) $ we have found vectors that almost satisfy the desired properties but they do not have the right dimension and we do not consider the standard scalar product yet. Without loss of generality let $ m \le n $. To obtain the required dimension we will project $ (y_j) $ orthogonally onto $ \textup{span}\{ x_1,...,x_m \} $.
	Let $ \{a_1,...,a_r\} $ be an orthonormal basis of $ \textup{span} \{ x_1,...,x_m \} $ with respect to $ \beta $. 
	The orthogonal projection of $ y_j $ is $ \pi(y):= \sum_{i=1}^{r}\beta(a_i,y_j)a_i $ and fulfills
	$ \beta(x_i,y_j) = \beta(x_i,\pi(y_j)) $. \textbf{perhaps explanation but standard calculation}
	Let $ x_i $ and $ \pi(y_j) $ admit the descriptions 
	$ x_i = \sum_{k=1}^{r}\alpha_k^{(i)}a_k$ and $  \pi(y_j) = \sum_{k=1}^r \gamma_k^{(j)} a_k$ for $ \alpha^{(i)}, \gamma^{(j)} \in \mathbb{R}^r $. Then
	\begin{align*}
		a_{ij}= \beta(x,y)= \beta(x,\pi(y)) = \sum_{1 \le k,l \le r} \alpha_k^{(i)} \gamma_l^{(j)} \beta(a_k,a_l) = \sum_{k=1}^{r}\alpha_k^{(i)}\gamma_k^{(j)} = \langle \alpha^{(i)}a, \gamma^{(j)} \rangle.
	\end{align*}
	Moreover, since $ \modul{x_i}, \, \modul{y_j}= \modul{\pi(y_j)} \le 1 $ we get $ \langle \alpha^{(i)}, \alpha^{(i)} \rangle, \, \langle \gamma^{(j)}, \gamma^{(j)} \rangle \le 1$ and thus the vectors $ (\alpha^{(i)}) $ and$ (\gamma^{(j)}) $ all required properties. 
	 
	 
	
\end{proof}
\begin{proof}[Proof of $ \LC_{m,n} \supset M $]
	Inhalt...
\end{proof}