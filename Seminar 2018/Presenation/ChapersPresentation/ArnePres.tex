\section{Local and quantum correlation matrices}
\subsection{Local correlation matrices }
\begin{frame}{Outline}
\tableofcontents[currentsection]
\end{frame}

\begin{frame}[label = LCMot]{Local correlation matrices}
	\begin{itemize}
		\item <1-> Deterministic strategies of Alice and Bob correspond to vectors $ \xi \in \{ -1,1 \}^\mathcal{S} $, respectively $ \eta \in \{ -1,1 \}^\mathcal{T} $
		\item <2-> Their common answer is the product $ \xi_i \eta_j $
		\item <3-> Instead of deterministic strategies they can also answer according to the probability distribution of random variables $ X_i, Y_j $
		\item <4-> Their common answer is $ \mathbb{E}[X_iY_j] $
		\item <5->This information can be encoded in an $ \mathcal{S} \times \mathcal{T} $ matrix 
		\hyperlink{QCMot<4>}{\beamerreturnbutton{Motivation Quantum}}
	\end{itemize} 
\end{frame}

\begin{frame}
	\begin{definition}
		Let $ (X_i)_{1 \le i \le m } $ and $ (Y_j)_{1 \le j \le n} $ be families of random variables on a common probability space such that $ \vert X_i \vert, \vert Y_j \vert \le 1 $ almost surely. Then $ A=(a_{ij}) $ is the corresponding {\itshape classical (or local) correlation matrix} if 
		\begin{align*}
		a_{ij} = \mathbb{E}[X_iY_j]
		\end{align*}
		for all $ 1 \le i \le m, 1 \le j \le n $.
	\end{definition}
	\pause
	\begin{itemize}
		\item Set of all local correlation matrices: $ \LC_{m,n} $
	\end{itemize}
	\pause
	\begin{lemma}
		\begin{align*}
			\LC_{m,n} = \textup{conv} \{  \xi\eta^T \, | \, \xi \in \{-1,1\}^m, \eta \in \{-1,1\}^n     \}
		\end{align*}
		
	\end{lemma}
	\begin{itemize}
		\pause
		\item No matter which probabilistic strategy there is a deterministic one which as at least as good as the one one chooses 
	\end{itemize}
\end{frame}

\begin{frame}
	\begin{proof}[$   \LC_{m,n}  \supset \textup{conv} \{  \xi\eta^T \, | \, \xi \in \{-1,1\}^m, \eta \in \{-1,1\}^n     \} $]
		\begin{itemize}
			 \item<1-> $ \xi \eta^T \in LC_{m,n}  $ for all $\xi \in \{-1,1\}^m, \eta \in \{-1,1\}^n   $   
			 	(Choose $ X_i \equiv \xi_i, \, Y_j \equiv \eta_j $)
			\item<2-> Suffices to show that $ \LC_{m,n} $ is convex. 
			\item <3->Let $ a_{ij}^{(k)} = \mathbb{E}[X_i^{(k)}Y_{j}^{(k)}] $ for $ k \in \{0,1\} $
			\item<4-> Find $ (X_i),(Y_j) $ with $ \modul{X_i},\modul{Y_j} \le 1 $ almost surely such that for $ \beta \in [0,1] $
			$ 	\beta a_{ij}^{(0)}+ (1-\beta)a_{ij}^{(1)} = \mathbb{E}[X_iY_j] $ 
			\item<5-> Define a Bernoulli random variable $ \alpha $ (which is independent form $ X_i^{(k)},Y_j^{(k)} $) such that $ \mathbb{P}(\alpha = 0) = \beta $, $ \mathbb{P}(\alpha = 1) = 1 - \beta$ and set $ X_i = X_i^{(\alpha)}, Y_j = Y_j^{(\alpha)} $
			\item<6-> Then 
			\begin{align*}
			\mathbb{E}[X_iY_j] &= \mathbb{E}[X_i^{(\alpha)}Y_j^{(\alpha)}  \mathds{1}_{ \{\alpha = 0\}}] + \mathbb{E}[X_i^{(\alpha)}Y_j^{(\alpha)}]\mathds{1}_{\{\alpha = 1\}}] \\
			&=\mathbb{E}[X_i^{(0)}Y_j^{(0)}]\mathbb{P}(\alpha = 0)+ \mathbb{E}[X_i^{(1)}Y_j^{(1)}]\mathbb{P}(\alpha = 1)  \\
			&= \beta a_{ij}^{(0)}+ (1-\beta) a_{ij}^{(1)}
			\end{align*}
		\end{itemize}
	\end{proof}
\end{frame}


\begin{frame}
\begin{block}{$   \LC_{m,n}  \subset \textup{conv} \{  \xi\eta^T \, | \, \xi \in \{-1,1\}^m, \eta \in \{-1,1\}^n     \} $}
	\begin{itemize}
		\item<1-> Let $ a_{ij} = \mathbb{E}[X_iY_j] $ for $ \mathbb{R} $-valued random variables $ (X_i),(Y_j) $ defined on a common probability space $ \Omega $ with $ \modul{X_i},\modul{Y_j} \le 1 $ almost surely. 
		\item <2->Set $ X= (X_1,...,X_m) $ and $ Y= (Y_1,...,Y_n) $, then $ X \in [-1,1]^m, \, Y \in [-1,1]^n $ almost surely.
		\item<3-> Hypercube description by its vertices: $ [-1,1]^d = \textup{conv} \{\xi \, | \, \xi \in \{-1,1 \}^d \}$ 
		\item<4-> Define random variables $ \lambda_{\xi}^{(X)}: \Omega^m \to [0,1] $ such that 
			\begin{align*}
			X(\omega) = \sum_{\xi \in \{-1,1\}^m}\lambda_{\xi}^{(X)}(\omega)\xi
			\end{align*} 
			almost surely 
			and $ \sum_{\xi \in \{-1,1\}^m}\lambda_{\xi}^{(X)}(\omega) = 1  $
	\end{itemize}
\end{block}
\end{frame}

\begin{frame}
	\begin{proof}[$   \LC_{m,n}  \subset \textup{conv} \{  \xi\eta^T \, | \, \xi \in \{-1,1\}^m, \eta \in \{-1,1\}^n     \} $]
		\begin{itemize}
			\item<1-> {\footnotesize Define random variables $ \lambda_{\xi}^{(X)}: \Omega^m \to [0,1] $ such that 
			\begin{align*}
			X(\omega) = \sum_{\xi \in \{-1,1\}^m}\lambda_{\xi}^{(X)}(\omega)\xi
			\end{align*} 
			almost surely 
			and $ \sum_{\xi \in \{-1,1\}^m}\lambda_{\xi}^{(X)}(\omega) = 1  $}
			\item<1-> Using the same decomposition for $ Y $ we obtain 
			\begin{align*}
			a_{ij} = \mathbb{E}[X_iY_j] &= \mathbb{E} \big [ \big (\sum_{\xi \in \{-1,1\}^m}\lambda_{\xi}^{(X)}\xi_i \big  ) \big (\sum_{\eta \in \{-1,1\}^n}\lambda_{\eta}^{(Y)}\eta_j \big ) \big ]   \\
			&= \sum_{\xi \in \{-1,1\}^m, \eta \in \{-1,1\}^n} \mathbb{E}\big [\lambda_{\xi}^{(X)}\lambda_{\eta}^{(Y)} \big ] \xi_i \eta_j  \\
			&= \sum_{\xi \in \{-1,1\}^m, \eta \in \{-1,1\}^n}\mathbb{E} [\lambda_{\xi}^{(X)} ]\mathbb{E}[\lambda_{\eta}^{(Y)}]\xi_i\eta_j
			\end{align*}
			\item<2-> Due to $ \sum_{\xi \in \{-1,1\}^m, \eta \in \{-1,1\}^n}\mathbb{E} [\lambda_{\xi}^{(X)} ]\mathbb{E}[\lambda_{\eta}^{(Y)}] = 1 $
			the matrix $ (a_{ij}) $ is a convex combination of $ \xi \eta^T $, $ \xi \in \{-1,1\}^m, \eta \in \{-1,1 \}^n $
		\end{itemize}
	\end{proof}
\end{frame}



\subsection{Quantum correlation matrices}

\begin{frame}[label = QCMot]{Quantum correlation matrices}
	\begin{itemize}
		\item<1-> Alice and Bob share a common state $ \rho $ and get inputs $ i \in \mathcal{S}, \, j \in \mathcal{T} $ 
		\item<2-> They perform measurements $ \{ F_s^{\xi} \}_{\xi = \pm 1} $, respectively $ \{ G_t^{\eta} \}_{\eta = \pm 1} $
		\item <3-> The probability that their response is $ (\xi,\eta) $ for inputs $ (i,j) $ is given by 
		$ a_{ij} = \trace{(\rho F_s^\xi \otimes G_t^\eta )} $
		\item<4-> Again we can encode this information in a matrix
	\end{itemize}
	\hyperlink{LCMot}{\beamerbutton{Motivation Locality}}
\end{frame}

\begin{frame}
	\begin{definition}
		Let $ (X_i)_{1 \le i \le m } $ and $ (Y_j)_{1 \le j \le n} $ be self-adjoint operators on $ \mathbb{C}^{d_1} $, respectively $ \mathbb{C}^{d_2} $ for some positive integers $ d_1,d_2 $, satisfying $ \norm{X_i}_{\infty}, \norm{Y_j}_{\infty} \le 1 $. $ A = (a_{ij}) $ is called {\itshape quantum correlation matrix} if there exists a state $ \rho $ on $\mathbb{C}^{d_1} \otimes \mathbb{C}^{d_2})$ such that 
		\begin{align*}\label{QCaij}
		a_{ij} = \trace{(\rho X_i \otimes Y_j)}.
		\end{align*}
	\end{definition}
	\pause
	\begin{itemize}
		\item Set of all quantum correlation matrices denoted by $ \QC_{m,n} $
		\pause
	\end{itemize}
	\begin{lemma}
		\begin{align*}
		\QC_{m,n} = \{ (\langle x_i,y_j \rangle)_{1 \le i \le m, 1 \le j \le n} \,| \, x_i,y_j \in \mathbb{R}^{ \min \{m,n \} }, \vert x_i  \vert \le 1, \vert y_j \vert \le 1  \}
		\end{align*}
	\end{lemma}
\end{frame}

\begin{frame}
\begin{block}{$ \QC_{m,n} \subset \{ (\langle x_i,y_j \rangle)_{1 \le i \le m, 1 \le j \le n} \,| \, x_i,y_j \in \mathbb{R}^{ \min \{m,n \} }, \vert x_i  \vert \le 1, \vert y_j \vert \le 1  \} $}
	\begin{itemize}
		\item<1-> $ a_{ij} = \trace{(\rho X_i \otimes Y_j)} $, state $ \rho $ on a Hilbert space $ \mathcal{H} = \mathbb{C}^{d_1} \otimes\mathbb{C}^{d_2} $ and Hermitian operators $ (X_i)_{1 \ge m}, \, (Y_j)_{1 \ge n} $ on $ \mathbb{C}^{d_1} $, respectively $ \mathbb{C}^{d_2} $ satisfying $ \norm{X_i}_{\infty}, \norm{Y_j}_{\infty} \le 1 $
		\item<2-> Define a positive semidefinite symmetric bilinear form on the space of Hermitian operators on $ \mathcal{H} $ by 
		$ \beta: \mathcal{H} \times \mathcal{H} \to \mathbb{R} $ where $ \beta(S,T) =\textup{Re}( \trace{\rho ST}) $.
		\item<3-> Obtain an inner product space $ U := B^{sa}(\mathcal{H}) / \ker \beta$ equipped with the inner product 
		\begin{align*}
			\tilde{\beta}([S],[T]) = \beta(S,T).
		\end{align*}
		\item<4-> Identify $ X_i \otimes I,I \otimes Y_j $ with vectors $ x_i,y_j $ in $ U $, then 
		\begin{align*}
			\tilde{\beta}(x_i,y_j) = \beta(X_i,Y_j) = Re \trace{(\rho X_i \otimes Y_j)} = a_{ij}
		\end{align*}
	\end{itemize}
\end{block}
\end{frame}
\begin{frame}
	\begin{proof}[$ \QC_{m,n} \subset \{ (\langle x_i,y_j \rangle)_{1 \le i \le m, 1 \le j \le n} \,| \, x_i,y_j \in \mathbb{R}^{ \min \{m,n \} }, \vert x_i  \vert \le 1, \vert y_j \vert \le 1  \} $]
		\begin{itemize}
			\item<1-> {\footnotesize Identify $ X_i \otimes I,I \otimes Y_j $ with vectors $ x_i,y_j $ in $ U $, then 
			\begin{align*}
			\tilde{\beta}(x_i,y_j) = \beta(X_i \otimes I,I \otimes Y_j) = \textup{Re} \trace{(\rho X_i \otimes Y_j)} = a_{ij}
			\end{align*}}
			\item<1-> $ \beta (X \otimes I, X \otimes I), \beta (I \otimes Y, I \otimes Y) \le 1$ \\
			(this can be shown by using a {\itshape Schmidt-decomposition} of $ \rho $ and using $ \norm{X_i}_{\infty}, \norm{Y_j}_{\infty} \le 1 $)
			\item<2-> Project the $ y_j $'s orthogonally onto $ \textup{span} \{ x_1,...,x_m \} $  (wlog $ m \le n $)
			\item<3-> If $ \pi(y_j) $ is the projection of $ y_j $ then $ \tilde{\beta}(x_i,\pi(y_j)) = \tilde{\beta}(x_i,y_j) $
			\item<4-> The vectors still have not the right dimension but again, we can project them onto vectors in $ \mathbb{R}^m $
		\end{itemize}
		
	\end{proof}
\end{frame}


\begin{frame}
	\begin{itemize}
		\item  In order to show 
		\begin{align*} \QC_{m,n} \supset \{ (\langle x_i,y_j \rangle)_{1 \le i \le m, 1 \le j \le n} \,| \, x_i,y_j \in 				\mathbb{R}^{ \min \{m,n \} }, \vert x_i  \vert \le 1, \vert y_j \vert \le 1  \} 
		\end{align*}  we will use the following 
	\end{itemize}
	\pause
	\begin{block}{Proposition}
		For all $ n \ge 1 $ there is a subspace of the $ 2^n \times 2^n $ Hermitian matrices where every element is the multiple of a unitary matrix. 
	\end{block}
	\pause
	\begin{itemize}
		\item The proof is based on $ n- $fold tensor products of the Pauli matrices which are the three matrices 
		\begin{align*}
		X = \begin{pmatrix}
		0 & 1 \\ 1 & 0
		\end{pmatrix}, \, Y = \begin{pmatrix}
		0 & -i \\ i & 0
		\end{pmatrix}, \, Z = \begin{pmatrix}
		1 & 0 \\ 0 & -1
		\end{pmatrix}, I = \begin{pmatrix}
		1 & 0 \\ 0 & 1
		\end{pmatrix}
		\end{align*}
	\end{itemize}
\end{frame}
\begin{frame}
	\begin{itemize}
		\item<1->{\footnotesize $
		X = \begin{pmatrix}
		0 & 1 \\ 1 & 0
		\end{pmatrix}, \, Y = \begin{pmatrix}
		0 & -i \\ i & 0
		\end{pmatrix}, \, Z = \begin{pmatrix}
		1 & 0 \\ 0 & -1
		\end{pmatrix}, I = \begin{pmatrix}
		1 & 0 \\ 0 & 1
		\end{pmatrix} $}
	\end{itemize}
		\begin{proof}
			\begin{itemize}
				\item Define 
				\begin{align*}
					U_i &= I^{\otimes (i-1)} \otimes X \otimes Y^{\otimes (n-i)},   \\
					U_{n+i} &= I^{\otimes (i-1)} \otimes Z \otimes Y^{\otimes (n-i)}, \, i=1,\hdots n 
				\end{align*}
				\item $ U_i $'s are anti-commuting traceless Hermitian unitaries, i.e.
				$ U_iU_j = -U_jU_i $ for $ i \neq j $ and $ U_i^2 = I $ 
				\item For $ X = \sum_{i = 1}^{2n}\xi_i U_i$, $ Y = \sum_{i = 1}^{2n} = \eta_iU_i $ we can calculate 
				\begin{align*}
				XY &= \sum_{i = 1}^{2n} \xi_i \eta_i I + \sum_{1 \le i,j \le \ 2n}\xi_i\eta_j U_i U_j   \\
				&= \sum_{i = 1}^{2n} \xi_i \eta_i I + \sum_{1 \le i < j \le \le 2n}\xi_i\eta_jU_iU_j - \sum_{1 \le i < j \le  2n}U_iU_j =\sum_{i = 1}^{2n} \xi_i \eta_i I \\
				&= \langle \xi, \eta \rangle I.
				\end{align*}
				\item The result follows by setting $ Y = X^{*} $.
			\end{itemize}
		\end{proof}
\end{frame}

\begin{frame}
	\begin{block}{$ \QC_{m,n} \supset \{ (\langle x_i,y_j \rangle)_{1 \le i \le m, 1 \le j \le n} \,| \, x_i,y_j \in \mathbb{R}^{ \min \{m,n \} }, \vert x_i  \vert \le 1, \vert y_j \vert \le 1  \} $}
		\begin{itemize}
			\item<1->  Let $ (x_i)_{1 \le i \le m}, \, (y_j)_{1 \le j \le n} \subset \mathbb{R}^{\min \{ m,n \}}$ such that $ \modul{x_i},$$ \modul{y_j} \le 1 $. 
			\item<2->  Let $ X_i = \sum_{k=1}^{\min \{m,n\}} x_i(k)U_i $ and $ Y_j^{T } = \sum_{k=1}^{\min \{m,n\}}y_j(k)U_k $ where the $ U_i $'s are $ d \times d $ matrices with $  d = 2^{\lceil \min \{m,n\}/2 \rceil} $
			\item<3-> $ \trace{(X_iY_j^T)} = d\cdot \langle x_i, y_j \rangle  $ and $ \norm{X_i}_{\infty} \le 1 $ since $ X_iX_i^* = \modul{x_i}^2I $ and $ \modul{x_i}^2 \le 1  $ (the same holds for $ Y_j $)
			\item<4-> Let $ \ket{\psi} = \frac{1}{\sqrt{d}}\sum_{i= 1}^{d}\ket{i}\otimes \ket{i} $ and $ \rho = \ket{\psi}\bra{\psi} $. Note that we can write $ \rho $ as
			\begin{align*}
			\rho = \ket{\psi}\bra{\psi} = \frac{1}{d}\sum_{1 \le k,l \le d}\ket{k}\otimes \ket{k}\bra{l}\otimes \bra{l} =  \frac{1}{d} \sum_{1 \le k,l \le d}\ket{k}\bra{l} \otimes \ket{k}\bra{l}
			\end{align*}
		\end{itemize}
	\end{block}
\end{frame}

\begin{frame}
	\begin{proof}[$ \QC_{m,n} \supset \{ (\langle x_i,y_j \rangle)_{1 \le i \le m, 1 \le j \le n} \,| \, x_i,y_j \in \mathbb{R}^{ \min \{m,n \} }, \vert x_i  \vert \le 1, \vert y_j \vert \le 1  \} $]
		\begin{itemize}
			\item<1->{\footnotesize Let $ \ket{\psi} = \frac{1}{\sqrt{d}}\sum_{i= 1}^{d}\ket{ii} $ and $ \rho = \ket{\psi}\bra{\psi} $. Note that we can write $ \rho $ as
			\begin{align*}
			\rho = \ket{\psi}\bra{\psi} = \frac{1}{d}\sum_{1 \le k,l \le d}\ket{kk}\bra{ll} = \frac{1}{d} \sum_{1 \le k,l \le d}\ket{k}\bra{l} \otimes \ket{k}\bra{l}
			\end{align*}}
			\item<1-> Then 
			\begin{align*}
			\trace{(\rho X_i \otimes Y_j)} &=\frac{1}{d} \sum_{1 \le k,l \le d} \trace{ \big (\ket{k}\bra{l}X_i \otimes \ket{k}\bra{l}Y_j \big )}   \\
			&= \frac{1}{d} \sum_{1 \le k,l \le d} \trace{\big (\ket{k}\bra{l}X_i \big )} \trace{\big (\ket{k}\bra{l}Y_j \big )} \\
			&=  \frac{1}{d} \trace{X_iY_j^T} = \langle x_i,y_j \rangle.
			\end{align*}
		\end{itemize}
	\end{proof}
\end{frame}


\begin{frame}
	\begin{itemize}
		\item<1-> $ \QC_{m,n} $ is convex
		\item<2->Let $a_{ij} = \langle x_i, y_j \rangle $ and $ \bar{a}_{ij} = \langle \bar{x}_i, \bar{y}_j \rangle $ for $ x_{i},y_j, \bar{x}_i,\bar{y}_j \in \mathbb{R}^{\min \{m,n\}} $ such that $ \modul{x_i},\modul{y_j}, \modul{\bar{x}_i}, \modul{\bar{y}_j} \le 1 $.
		\item<3-> define vectors $\tilde{x}_i:= \begin{pmatrix}
		\sqrt{\lambda}x_i  \\ \sqrt{1-\lambda}\bar{x_i}
		\end{pmatrix}, \tilde{y}_j:= \begin{pmatrix}
		\sqrt{\lambda}y_j  \\ \sqrt{1-\lambda}\bar{y_j}
		\end{pmatrix} $ for $ \lambda \in [0,1] $
		\item<4-> it holds $ \modul{\tilde{x}_i} \le \lambda \modul{\begin{pmatrix}
			x_i \\ 0
			\end{pmatrix}} + (1-\lambda)\modul{\begin{pmatrix}
			0 \\ \bar{x_i}
			\end{pmatrix}} \le 1 $ and $ \langle \tilde{x}_i, \tilde{y}_j \rangle = \lambda \langle x_i,y_j \rangle + (1-\lambda) \langle \tilde{x_i},\tilde{y}_j \rangle$.
		\item<5-> Right dimension is obtained by projecting $ (\tilde{x}_i)_{1 \le i \le m}, \, (\tilde{y}_j)_{1 \le i \le n} $ on $\textup{span}\{x_1,\hdots,x_m\} $ or $\textup{span}\{y_1,...,y_n\} $, as in the proof before.
	\end{itemize}
\end{frame}
\subsection{The relations between quantum correlation and local correlation matrices}

\begin{frame}{The relations between quantum correlation and local correlation matrices}
	\begin{itemize}
		\item<1->{\footnotesize  $ \LC_{m,n} = \textup{conv} \{  \xi\eta^T \, | \, \xi \in \{-1,1\}^m, \eta \in \{-1,1\}^n     \} $}
		\item<1-> {\footnotesize $ \QC_{m,n} = \{ (\langle x_i,y_j \rangle)_{1 \le i \le m, 1 \le j \le n} \,| \, x_i,y_j \in \mathbb{R}^{ \min \{m,n \} }, \vert x_i  \vert \le 1, \vert y_j \vert \le 1  \} $ }
		\item<1-> $ \LC_{m,n} \subset \QC_{m,n} $
		\item<2-> Set $ x_i = \xi_i \ket{1}$ and $ y_j = \eta_j\ket{1} $ it immediately follows  $ \xi_i\eta_j = \langle x_i, y_j \rangle $. Hence, $ \xi\eta^T \in \QC_{m,n} $ (rest follows with the convexity of $ \QC_{m,n} $)
		\item<3-> Inclusion is strict in general 
	\end{itemize}
\end{frame}

\begin{frame}
	\begin{itemize}
		\item<1-> Let us consider the case $ n=m=2 $.
		\item<2-> {\footnotesize  $ \LC_{2,2} = \{ \xi \eta^T \, | \, \xi \in \{ -1,1 \}^2, \, \eta \in \{ -1,1 \}^2 \} $}
		\item <2-> It holds 
		\begin{align*}
		\LC_{2,2}= \textup{conv} \{ \pm \begin{pmatrix}
		1 & 1 \\
		1 & 1
		\end{pmatrix} , \, \pm \begin{pmatrix}
		-1 & -1 \\
		1 & 1
		\end{pmatrix} , \, \pm \begin{pmatrix}
		-1 & 1 \\
		-1 & 1
		\end{pmatrix}, \, \pm \begin{pmatrix}
		-1 & 1 \\
		1 & -1
		\end{pmatrix}  \}.
		\end{align*}
		\item<3->
		W can also write it as in intersections of halfspaces: 
			\begin{equation}\label{facetsLC}
			\LC_{2,2} = \{ A \in \mathbb{R}^{2 \times 2} \, | \, -1 \le  \trace{AM} \le 1 \textup{ for all } M \in \mathcal{K} \},
			\end{equation}
			where $ \mathcal{K}  = \{ \frac{1}{2}\sigma(\begin{pmatrix}
			-1 & 1 \\
			1 & 1
			\end{pmatrix}), \sigma(\begin{pmatrix}
			1 & 0 \\
			0 & 0
			\end{pmatrix}) \, | \,  \sigma \in \{ \textup{id}, (1 \, \, 2), (1 \, \, 3), (1 \, \, 4) \} \} $.
		\end{itemize}
\end{frame}


\begin{frame}
	\begin{itemize}
		\item<1-> We will show that the inclusion is strict by showing that both sets yield different values if we maximize in a certain direction
		\item<2-> Let $M = \begin{pmatrix}
		1 & 1 \\ 1 & -1 
		\end{pmatrix} $
		\item <3-> {\footnotesize  $
			\LC_{2,2} = \{ A \in \mathbb{R}^{2 \times 2} \, | \, -1 \le  \trace{AM} \le 1 \textup{ for all } M \in \mathcal{K} \},
			$
			where $ \mathcal{K}  = \{ \frac{1}{2}\sigma(\begin{pmatrix}
			-1 & 1 \\
			1 & 1
			\end{pmatrix}), \sigma(\begin{pmatrix}
			1 & 0 \\
			0 & 0
			\end{pmatrix}) \, | \,  \sigma \in \{ \textup{id}, (1 \, \, 2), (1 \, \, 3), (1 \, \, 4) \} \} $.}
		\item<3-> $ \max \{  \trace{(AM)}\, | \, A \in \LC_{2,2} \} = 2 $
			 
		
	\end{itemize}
\end{frame}

\begin{frame}
\begin{itemize}
	\item<1-> {\footnotesize $ \max \{  \trace{(AM)}\, | \, A \in \LC_{2,2} \} = 2 $ }
	\item<2-> Recall $ QC_{2,2} = \{ \langle x_i,y_j \rangle \, | \, x_1,x_2,y_1,y_2 \in \mathbb{R}^2, \, \modul{x_i},\modul{y_j} \le 1 \} $
	\item <4->$ A \in \QC_{2,2} $ we obtain, by Cauchy-Schwarz and $ \modul{y_i}\le 1 $,
	\begin{align*}
	\trace{(AM)}&= \langle x_1,y_1 \rangle + \langle x_1,y_2 \rangle + \langle x_2,y_1 \rangle - \langle x_2,y_2 \rangle   \\
	&= \langle x_1+x_2,y_1 \rangle + \langle x_1-x_2,y_2 \rangle  \le \modul{x_1+x_2}\modul{y_1} + \modul{x_1-x_2}\modul{y_2}  \\
	&\le \modul{x_1+x_2} + \modul{x_1-x_2}.
	\end{align*} 
	\item<5-> $(\modul{x_1+x_2} + \modul{x_1-x_2})^2 \le  4(\modul{x_1}^2+\modul{x_2}^2) $
	\item<6-> $\trace{(AM)} \le \modul{x_1+x_2} + \modul{x_1-x_2} \le 2\sqrt{\modul{x_1}^2+\modul{x_2}^2} \le 2 \sqrt{2}. $
	\item<7-> Bound is achieved by  $ A = \frac{1}{\sqrt{2}}\begin{pmatrix}
	1 & 1 \\ 1 & 1 
	\end{pmatrix} $, induced by the vectors $ x_1 = x_2 = \frac{1}{\sqrt{2}}(1,1) $ and $ y_1 = y_2 =(1,0) $
\end{itemize}
\end{frame}



